artifacts:
  ARTIFACTS_DIR: artifacts
  DATA_DIR: artifacts/data
  RETRIEVER_DIR: artifacts/retriever
  READER_DIR: artifacts/reader
  INDEX_DIR: artifacts/index

data:
  # Primary dataset – change later to your own benchmark
  dataset_name: ashaduzzaman/LongSum-2025
  dataset_config: default
  dataset_split: train[:100]   # Small for testing – change to train/validation later
  max_samples: null
  max_input_length: 100000
  chunk_size: 512
  chunk_overlap: 64
  top_k: 32

  longsum_2025:
    datasets:
      - name: booksum
        hf_name: kmfoda/booksum
        config: null  # No config needed
        text_key: chapter
        summary_key: summary
        domain: literature
        train_samples: 100
        val_samples: 30
        test_samples: 20

      - name: arxiv
        hf_name: ccdv/arxiv-summarization
        config: document
        text_key: article
        summary_key: abstract
        domain: scientific
        train_samples: 2000
        val_samples: 500
        test_samples: 250

      - name: govreport
        hf_name: ccdv/govreport-summarization
        config: null  # Valid config for GovReport (use 3 for full docs)
        text_key: report
        summary_key: summary
        domain: government
        train_samples: 1000
        val_samples: 300
        test_samples: 200

      - name: pubmed
        hf_name: ccdv/pubmed-summarization
        config: document
        text_key: article
        summary_key: abstract
        domain: medical
        train_samples: 1500
        val_samples: 400
        test_samples: 200

      - name: qmsum
        hf_name: pszemraj/qmsum-cleaned
        config: null  # No config needed
        text_key: input
        summary_key: output
        domain: meetings
        train_samples: 600
        val_samples: 150
        test_samples: 100

    bertscore_threshold: 0.72
    output_dir: artifacts/longsum_2025

retriever:
  model_name: BAAI/bge-large-en-v1.5
  device: cuda
  batch_size: 32

reader:
  base_model: meta-llama/Meta-Llama-3.1-8B-Instruct
  lora_r: 64
  lora_alpha: 16
  target_modules: ["q_proj", "v_proj"]
  max_seq_length: 32768
  use_4bit: true
  use_gradient_checkpointing: true
